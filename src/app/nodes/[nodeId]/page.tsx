import { Globe, Server, Database, Code, Shield, Bot } from 'lucide-react'
import { redirect, notFound } from 'next/navigation'
import React from 'react'
import { NodePageClient } from './NodePageClient'

// This ensures all possible paths are generated at build time
export async function generateStaticParams() {
    // Only generate params for nodes handled by this dynamic page
    const validNodeIds = Object.keys(nodesInfo).filter(
        nodeId => !['inference', 'llm_service', 'vector_db', 'training', 'security'].includes(nodeId)
    );
    return validNodeIds.map(nodeId => ({
        nodeId,
    }));
}

// Define the node info type with string instead of React.ElementType
interface NodeInfo {
    id: string
    label: string
    description: string
    icon: string // Changed from React.ElementType to string
    color: string
}

// Define the vulnerability type
interface Vulnerability {
    id: string
    title: string
    description: string
    position: { node: string; offset: { x: number; y: number } }
    color: string
    path: string
}

interface VulnerabilityCardProps {
    id: string
    title: string
    description: string
    color: string
    path: string
}

// Node information with string icons
const nodesInfo: { [key: string]: NodeInfo } = {
    client: {
        id: 'client',
        label: 'Client/Malicious Actor',
        description: 'The client or malicious actor who interacts with the LLM system, potentially attempting to exploit vulnerabilities.',
        icon: 'Globe',
        color: '#00ffff'
    },
    inference: {
        id: 'inference',
        label: 'Ingress',
        description: 'The entry point for user inputs to the LLM system, handling queries before processing.',
        icon: 'Bot',
        color: '#3b82f6'
    },
    llm_service: {
        id: 'llm_service',
        label: 'LLM Service',
        description: 'The core language model service that processes inputs and generates responses.',
        icon: 'Server',
        color: '#ff00ff'
    },
    vector_db: {
        id: 'vector_db',
        label: 'Vector DB',
        description: 'Database storing vector embeddings used by the LLM for retrieval-augmented generation.',
        icon: 'Database',
        color: '#22c55e'
    },
    training: {
        id: 'training',
        label: 'Training Pipeline',
        description: 'The pipeline responsible for training and fine-tuning the language model.',
        icon: 'Code',
        color: '#eab308'
    },
    security: {
        id: 'security',
        label: 'Security Layer',
        description: 'The security mechanisms that protect the LLM system from various threats.',
        icon: 'Shield',
        color: '#ef4444'
    }
};

// All vulnerabilities from the diagram
const allVulnerabilities: Vulnerability[] = [
    {
        id: 'LLM01',
        title: 'Prompt Injection',
        description: 'Manipulating LLMs via crafted inputs can lead to unauthorized access, data breaches, and compromised decision-making.',
        position: { node: 'inference', offset: { x: -400, y: -80 } },
        color: '#3b82f6',
        path: '/labs/prompt-injection'
    },
    {
        id: 'LLM02',
        title: 'Sensitive Information Disclosure',
        description: 'Sensitive information can affect both the LLM and its application context. This includes personal identifiable information (PII), financial details, health records, confidential business data, security credentials, and legal documents.',
        position: { node: 'llm_service', offset: { x: 200, y: -80 } },
        color: '#ff00ff',
        path: '/labs/sensitive-info-disclosure'
    },
    {
        id: 'LLM03',
        title: 'Supply Chain',
        description: 'LLM supply chains are susceptible to various vulnerabilities, which can affect the integrity of training data, models, and deployment platforms. These risks can result in biased outputs, security breaches, or system failures.',
        position: { node: 'training', offset: { x: -200, y: 0 } },
        color: '#eab308',
        path: '/labs/supply-chain'
    },
    {
        id: 'LLM04',
        title: 'Data and Model Poisoning',
        description: 'Data poisoning occurs when pre-training, fine-tuning, or embedding data is manipulated to introduce vulnerabilities, backdoors, or biases.',
        position: { node: 'vector_db', offset: { x: 200, y: -80 } },
        color: '#22c55e',
        path: '/labs/data-poisoning'
    },
    {
        id: 'LLM05',
        title: 'Improper Output Handling',
        description: 'Improper Output Handling refers specifically to insufficient validation, sanitization, and handling of the outputs generated by large language models before they are passed downstream to other components and systems.',
        position: { node: 'inference', offset: { x: 200, y: -80 } },
        color: '#3b82f6',
        path: '/labs/improper-output'
    },
    {
        id: 'LLM06',
        title: 'Excessive Agency',
        description: 'Granting LLMs unchecked autonomy to take action can lead to unintended consequences, jeopardizing reliability, privacy, and trust.',
        position: { node: 'llm_service', offset: { x: -200, y: 150 } },
        color: '#ff00ff',
        path: '/labs/excessive-agency'
    },
    {
        id: 'LLM07',
        title: 'System Prompt Leakage',
        description: 'The system prompt leakage vulnerability in LLMs refers to the risk that the system prompts or instructions used to steer the behavior of the model can also contain sensitive information that was not intended to be discovered',
        position: { node: 'security', offset: { x: -200, y: 80 } },
        color: '#ef4444',
        path: '/labs/system-prompt-leakage'
    },
    {
        id: 'LLM08',
        title: 'Vector and Embedding Weaknesses',
        description: 'Weaknesses in how vectors and embeddings are generated, stored, or retrieved can be exploited by malicious actions (intentional or unintentional) to inject harmful content, manipulate model outputs, or access sensitive information.',
        position: { node: 'vector_db', offset: { x: 200, y: 80 } },
        color: '#22c55e',
        path: '/labs/vector-embedding-weakness'
    },
    {
        id: 'LLM09',
        title: 'Misinformation',
        description: 'Misinformation occurs when LLMs produce false or misleading information that appears credible.',
        position: { node: 'llm_service', offset: { x: 200, y: 150 } },
        color: '#ff00ff',
        path: '/labs/misinformation'
    },
    {
        id: 'LLM10',
        title: 'Unbounded Consumption',
        description: 'Unbounded Consumption occurs when a Large Language Model (LLM) application allows users to conduct excessive and uncontrolled inferences, leading to risks such as denial of service (DoS), economic losses, model theft, and service degradation',
        position: { node: 'inference', offset: { x: -200, y: -180 } },
        color: '#3b82f6',
        path: '/labs/unbounded-consumption'
    }
];

// Define the correct params type for Next.js pages
// interface PageParams {
//     params: {
//         nodeId: string;
//     };
// }

// Type for the props, with params wrapped in a Promise
type NodePageProps = {
    params: Promise<{ nodeId: string }>;
    searchParams?: Promise<{ [key: string]: string | string[] | undefined }>;
}

export default async function NodePage({ params: paramsPromise }: NodePageProps) {
    // Await the params promise to get the actual params object
    const params = await paramsPromise;
    const { nodeId } = params;

    // Handle redirects for dedicated node pages
    const dedicatedNodes = ['inference', 'llm_service', 'vector_db', 'training', 'security'];
    if (dedicatedNodes.includes(nodeId)) {
        redirect(`/nodes/${nodeId}`);
    }

    // Get node info
    const nodeInfo = nodesInfo[nodeId];

    // Handle invalid node IDs
    if (!nodeInfo) {
        notFound();
    }

    // Get related vulnerabilities
    const relatedVulnerabilities = allVulnerabilities.filter(
        vuln => vuln.position.node === nodeId
    );

    // Convert string icon to component name
    const iconName = nodeInfo.icon;

    return (
        <NodePageClient
            nodeInfo={nodeInfo}
            iconName={iconName}
            relatedVulnerabilities={relatedVulnerabilities}
        />
    );
} 